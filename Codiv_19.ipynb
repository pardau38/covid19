{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Codiv_19.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5xtJPGUGWl38",
        "1fdxIKzXWwDC",
        "DLjXUeiJW6K3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pardau38/covid19/blob/master/Codiv_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7guk4Q9E_4gh",
        "colab_type": "text"
      },
      "source": [
        "## General info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNwxHgr-_8uy",
        "colab_type": "text"
      },
      "source": [
        "[trello board](https://trello.com/b/ED3H13vT/covid-19-kaggle-kickoff)\n",
        "\n",
        "forked [notebook](https://www.kaggle.com/davidmezzetti/cord-19-analysis-with-sentence-embeddings)\n",
        "\n",
        "List of interesting notebook:\n",
        "\n",
        "[Explored drugs being develloped](https://www.kaggle.com/maria17/cord-19-explore-drugs-being-developed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sbfqgVO9Npa",
        "colab_type": "text"
      },
      "source": [
        "## Kaggle dataset and word vectors downloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2s1OVlL-QZ_",
        "colab_type": "text"
      },
      "source": [
        "Update your kaggle.json API key with the cell below, then launch the newt two.\n",
        "\n",
        "It'll download data if needed (*eg*, your kernel has restarted)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhf1L5gY_faQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload your kaggle API token\n",
        "from google.colab import files\n",
        "# In kaggle.com : MyAccount -> Create New API Token, will download kaggle.json that you can upload here.\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NouqN2I_H1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isfile(\"kaggle.json\") and not os.path.isdir(os.path.expanduser(\"~/.kaggle\")):\n",
        "  raise Exception(\"Please import your kaggle key first.\")\n",
        "\n",
        "if os.path.isfile(\"kaggle.json\"):\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "  !chmod 600 ~/.kaggle/kaggle.json\n",
        "  !rm kaggle.json\n",
        "\n",
        "if not os.path.isdir(\"kaggle_data\"):\n",
        "  # !kaggle datasets list | head\n",
        "  !pip install -q kaggle\n",
        "  !pip install -q kaggle-cli\n",
        "\n",
        "  !kaggle datasets download -d allen-institute-for-ai/CORD-19-research-challenge\n",
        "  !mkdir kaggle_data\n",
        "  !unzip -qq CORD-19-research-challenge.zip -d kaggle_data\n",
        "  !rm CORD-19-research-challenge.zip\n",
        "\n",
        "if not os.path.isdir(\"glove_vectors\"):\n",
        "  !pip install -q kaggle\n",
        "  !pip install -q kaggle-cli\n",
        "\n",
        "  !kaggle datasets download -d rtatman/glove-global-vectors-for-word-representation\n",
        "  !mkdir glove_vectors\n",
        "  !unzip -qq glove-global-vectors-for-word-representation.zip -d glove_vectors\n",
        "  !rm glove-global-vectors-for-word-representation.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVXJYpCcSafq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK data for pre-processing\n",
        "if not os.path.isdir(\"/root/nltk_data\"):\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  nltk.download('punkt')\n",
        "\n",
        "if \"corpora\" not in os.listdir(\"/root/nltk_data\"):\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "if \"tokenizers\" not in os.listdir(\"/root/nltk_data\"):\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "\n",
        "# Python packages\n",
        "try:\n",
        "  from retry import retry\n",
        "except ModuleNotFoundError:\n",
        "  !pip install retry\n",
        "\n",
        "try:\n",
        "  import pathos\n",
        "except ModuleNotFoundError:\n",
        "  !pip install pathos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEAuaVHJUdv7",
        "colab_type": "text"
      },
      "source": [
        "## Link your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PKGJlRMSV_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload files to your google drive (SQLite file eg) and mount it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouKnw314-3cb",
        "colab_type": "text"
      },
      "source": [
        "## Libs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj9S6PPl_pjM",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Fbnf7q9IiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import tqdm\n",
        "import time\n",
        "import pickle\n",
        "import sqlite3\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import multiprocessing as mp\n",
        "\n",
        "from datetime import date\n",
        "from retry import retry\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from dateutil import parser\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Generator, Tuple, Union\n",
        "from collections import OrderedDict, Counter, MutableMapping, Sequence\n",
        "from pathos.multiprocessing import ProcessingPool as picklable_pool\n",
        "from textblob import TextBlob\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import sent_tokenize, RegexpTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xtJPGUGWl38",
        "colab_type": "text"
      },
      "source": [
        "### File processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ5UnYXjWmPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\" Open JSON file and return dict() data \"\"\"\n",
        "    with open(file_path, \"r\") as handler:\n",
        "        json_data = json.loads(handler.read(), object_pairs_hook=OrderedDict)\n",
        "    return json_data\n",
        "\n",
        "\n",
        "def get_body(json_data: Dict[str, Any]) -> str:\n",
        "    \"\"\" Return body from json data \"\"\"\n",
        "    return \" \".join([json_data[\"body_text\"][index][\"text\"].strip() for index in range(len(json_data[\"body_text\"]))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnl8qj9QWrRm",
        "colab_type": "text"
      },
      "source": [
        "### Language detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8kpcSk9WrYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lang(text: str) -> str:\n",
        "  \"\"\" Detects language of text : must contain minimum 3 characters \"\"\"\n",
        "  if len(text) >= 3:\n",
        "    b = TextBlob(text)\n",
        "    return b.detect_language()\n",
        "  else:\n",
        "    raise ValueError('Minimum of 3 characters needed !')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fdxIKzXWwDC",
        "colab_type": "text"
      },
      "source": [
        "### Database utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjEeIyKaWwKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def instanciate_sql_db(db_path: str = \"articles_database.sqlite\") -> None:\n",
        "    \"\"\" Create an SQLite database \"\"\"\n",
        "\n",
        "    if os.path.isfile(db_path):\n",
        "        os.remove(db_path)    \n",
        "    database = sqlite3.connect(db_path)\n",
        "    # Storing articles\n",
        "    articles_table = {\n",
        "        \"paper_doi\": \"TEXT PRIMARY KEY\",\n",
        "        \"date\": \"DATETIME\",\n",
        "        \"body\": \"TEXT\",\n",
        "        \"abstract\": \"TEXT\",\n",
        "        \"title\": \"TEXT\",\n",
        "        \"sha\": \"TEXT\",\n",
        "        \"folder\": \"TEXT\"\n",
        "    }\n",
        "    columns = [\"{0} {1}\".format(name, col_type) for name, col_type in articles_table.items()]\n",
        "    command = \"CREATE TABLE IF NOT EXISTS articles ({});\".format(\", \".join(columns))\n",
        "    database.execute(command)\n",
        "    # Storing sentences\n",
        "    sentences_table = {\n",
        "        \"paper_doi\": \"TEXT\",\n",
        "        \"section\": \"TEXT\",\n",
        "        \"raw_sentence\": \"TEXT\",\n",
        "        \"sentence\": \"TEXT\",\n",
        "        \"vector\": \"TEXT\"\n",
        "    }\n",
        "    columns = [\"{0} {1}\".format(name, col_type) for name, col_type in sentences_table.items()]\n",
        "    command = \"CREATE TABLE IF NOT EXISTS sentences ({});\".format(\", \".join(columns))\n",
        "    database.execute(command)\n",
        "    database.close()\n",
        "\n",
        "def get_articles_to_insert(articles_df: pd.DataFrame) -> List[Any]:\n",
        "    \"\"\" List comprehension get stuck, who knows why \"\"\"\n",
        "    articles = []\n",
        "    for index, data in articles_df.iterrows():\n",
        "        articles.append((index, data))\n",
        "    return articles\n",
        "  \n",
        "@retry(sqlite3.OperationalError, tries=5, delay=2)\n",
        "def insert_row(list_to_insert: List[Any], table_name: str = \"articles\", db_path: str = \"articles_database.sqlite\") -> None:\n",
        "    \"\"\" Insert row of articles into the SQLite database \"\"\"\n",
        "\n",
        "    if table_name == \"articles\":\n",
        "        command = \"INSERT INTO articles(paper_doi, title, body, abstract, date, sha, folder) VALUES (?, ?, ?, ?, ?, ?, ?)\"\n",
        "    elif table_name == \"sentences\":\n",
        "        command = \"INSERT INTO sentences(paper_doi, section, raw_sentence, sentence, vector) VALUES (?, ?, ?, ?, ?)\"\n",
        "    else:\n",
        "        raise Exception(f\"Unknown table {table_name}\")\n",
        "\n",
        "    connection = sqlite3.connect(db_path)\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(command, list_to_insert)  # This line will be retried if fails\n",
        "    cursor.close()\n",
        "    connection.commit()\n",
        "    connection.close()\n",
        "\n",
        "def insert_article(args: Any) -> None:\n",
        "    \"\"\" Parse and insert a single article into the SQLite DB. args = [(index, df_line), db_path] \"\"\"\n",
        "    index = args[0][0]\n",
        "    data = args[0][1]\n",
        "    db_path = args[1]\n",
        "\n",
        "    # Get body\n",
        "    if data.has_full_text is True:\n",
        "        json_file = os.path.join(os.sep, \"kaggle\", \"input\", \"CORD-19-research-challenge\", data.full_text_file, data.full_text_file, f\"{data.sha}.json\")\n",
        "        try:\n",
        "            json_data = read_file(json_file)\n",
        "            body = get_body(json_data=json_data)\n",
        "            folder = data.full_text_file\n",
        "        except FileNotFoundError:\n",
        "            body = None\n",
        "            folder = None\n",
        "    else:\n",
        "        body = None\n",
        "        folder = None\n",
        "\n",
        "    try:\n",
        "        date = parser.parse(data.publish_time)\n",
        "    except Exception:  # Better to get no date than a string of whatever\n",
        "        date = None\n",
        "        \n",
        "    raw_data = [\n",
        "        data.doi,\n",
        "        data.title,\n",
        "        body,\n",
        "        data.abstract,\n",
        "        date,\n",
        "        data.sha,\n",
        "        folder\n",
        "    ]\n",
        "    insert_row(list_to_insert=raw_data, db_path=db_path)\n",
        "\n",
        "def get_all_ids(db_path: str = \"articles_database.sqlite\") -> List[str]:\n",
        "    \"\"\" Return all articles DOI stored in the article table \"\"\"\n",
        "    connection = sqlite3.connect(db_path)\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"SELECT paper_doi FROM articles\")\n",
        "    ids = cursor.fetchall()\n",
        "    cursor.close()\n",
        "    connection.close()\n",
        "    ids_cleaneds = [id_[0] for id_ in ids if len(id_) == 1]\n",
        "\n",
        "    return ids_cleaneds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfkKTAYNW1c1",
        "colab_type": "text"
      },
      "source": [
        "### Text pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AygcyhWW1ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text: str, stem_words: bool = True, remove_num: bool = True) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\" Pre-process extracted texts \"\"\"\n",
        "\n",
        "    word = RegexpTokenizer(r\"\\w+\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    \n",
        "    def filter_stopwords(sentence: List[str], stopwords: List[str] = stop_words) -> List[str]:\n",
        "        \"\"\" Remove stopwords from a given list of words \"\"\"\n",
        "        return [word for word in sentence if word not in stopwords]\n",
        "    \n",
        "    def stem_words(sentence: List[str], stem_function: Any = stemmer) -> List[str]:\n",
        "        \"\"\" Get words root for every member of an input list \"\"\"\n",
        "        return [stem_function.stem(word) for word in sentence]\n",
        "    \n",
        "    def remove_numeric_words(sentence: List[str]) -> List[str]:\n",
        "        \"\"\" Remove number (items) from a list of words \"\"\"\n",
        "        letter_pattern = re.compile(r\"[a-z]\")\n",
        "        return [word for word in sentence if letter_pattern.match(word)]   \n",
        "\n",
        "    # Split paragraphs into sentences and keep them for nive output\n",
        "    sentences_raw = sent_tokenize(text)\n",
        "    # Lower\n",
        "    sentences = [sentence.lower() for sentence in sentences_raw]\n",
        "    # Split sentences into words and remove punctuation\n",
        "    sentences = [word.tokenize(sentence) for sentence in sentences]\n",
        "    # Remove stopwords\n",
        "    sentences = [filter_stopwords(sentence) for sentence in sentences]\n",
        "    if stem_words is True:\n",
        "        # Stem words\n",
        "        sentences = [stem_words(sentence) for sentence in sentences]\n",
        "    if remove_num is True:\n",
        "        sentences = [remove_numeric_words(sentence) for sentence in sentences]\n",
        "    # Filter empty sentences and one-letters words\n",
        "    sentences = [[word for word in sentence if len(word) > 1] for sentence in sentences if sentence != []]\n",
        "    return sentences, sentences_raw\n",
        "\n",
        "def pre_process_articles(args: List[Any]) -> None:\n",
        "    \"\"\" Apply preprocessing to texts and store result into the SQLite DB \"\"\"\n",
        "\n",
        "    article_id: str = args[0]\n",
        "    embedding_model = args[1]\n",
        "    db_path: str = args[2]\n",
        "    stem_words: bool = args[3]\n",
        "    remove_num: bool = args[4]\n",
        "\n",
        "    connection = sqlite3.connect(db_path)\n",
        "    cursor = connection.cursor()\n",
        "    cursor.execute(\"SELECT * FROM articles WHERE paper_doi = ?\", [article_id])\n",
        "    # Get dict {column: value}\n",
        "    try:\n",
        "      article = {[col for col in head if col is not None][0]: value for head, value in zip(cursor.description, cursor.fetchone())}\n",
        "      cursor.close()\n",
        "      connection.close()\n",
        "    except TypeError:  # When the DB doest not return a result\n",
        "      cursor.close()\n",
        "      connection.close()\n",
        "      return None\n",
        "    \n",
        "    for section in [\"title\", \"abstract\", \"body\"]:\n",
        "        if article[section] is not None:\n",
        "            pp_sentences, sentences_raw = preprocess_text(article[section], stem_words=stem_words, remove_num=remove_num)\n",
        "            for pp_sentence, raw_sentence in zip(pp_sentences, sentences_raw):\n",
        "              try:\n",
        "                # paper, section, sentence, vector\n",
        "                row_to_insert = [\n",
        "                    article_id,\n",
        "                    section,\n",
        "                    raw_sentence,                                                                          # Raw sentence\n",
        "                    json.dumps(pp_sentence),                                                               # Store list of tokens as loadable str\n",
        "                    json.dumps([str(x) for x in embedding_model.compute_sentence_vector(pp_sentence)])     # Embeded vector\n",
        "                ]\n",
        "                try:\n",
        "                  insert_row(list_to_insert=row_to_insert, table_name=\"sentences\", db_path=db_path)\n",
        "                except sqlite3.OperationalError:  # Even the retry() decorator failed\n",
        "                  continue\n",
        "              except TypeError:  # When all words are not in the model\n",
        "                continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLjXUeiJW6K3",
        "colab_type": "text"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfHSRlW1W6Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding():\n",
        "\n",
        "    def __init__(self, vectors_path: str = None, embeddings_dimension: int = 50, sentence_embedding_method: str = \"mowe\"):\n",
        "        \n",
        "        if vectors_path is None:\n",
        "            self.vectors_path = os.path.join(os.sep, \"kaggle\", \"input\", \"glove-global-vectors-for-word-representation\", f\"glove.6B.{embeddings_dimension}d.txt\")\n",
        "        else:\n",
        "            self.vectors_path = vectors_path\n",
        "        \n",
        "        self.embeddings_dimension = embeddings_dimension\n",
        "        self.sentence_embedding_method = sentence_embedding_method\n",
        "\n",
        "    def build_vectors_dictionary(self) -> Dict[str, List[float]]:\n",
        "        \"\"\" Load pre-trained vectors and build a dict \"\"\"\n",
        "\n",
        "        tic = time.time()    \n",
        "\n",
        "        self.vectors = {}\n",
        "        with open(self.vectors_path, \"r\") as handler:\n",
        "            for line in handler.readlines():\n",
        "                try:\n",
        "                    # Prevent to keep useless words (otherwise pre-proc return nothing)\n",
        "                    word, word_raw = preprocess_text(line.split()[0])\n",
        "                    vector = [float(dimension) for dimension in line.split()[1:None]]\n",
        "                    assert len(vector) == self.embeddings_dimension\n",
        "                    self.vectors[word[0][0]] = vector\n",
        "                except IndexError:  # When the preprocessing does not return a word (useless word)\n",
        "                    continue     \n",
        "\n",
        "        toc = time.time()\n",
        "        print(f\"Took {round((toc-tic) / 60, 2)} min to load {len(self.vectors.keys())} GloVe vectors (embedding dim: {self.embeddings_dimension}).\")\n",
        "        \n",
        "    def compute_sentence_vector(self, sentence: List[str], sentence_embedding_method: str = \"mowe\") -> List[float]:\n",
        "        \"\"\" Compute a SOWE/MOWE over all tokens composing a sentence. Word skipped if not in model. \"\"\"\n",
        "        words_vector = [self.vectors[word] if word in self.vectors.keys() else list(list(np.full([1, EMBEDDING_DIMENSION, ], np.nan))[0]) for word in sentence]\n",
        "        if self.sentence_embedding_method == \"mowe\":\n",
        "            sentence_embedding = np.nanmean(words_vector, axis=0)\n",
        "        elif self.sentence_embedding_method == \"sowe\":\n",
        "            sentence_embedding = np.nansum(words_vector, axis=0)\n",
        "        else:\n",
        "            raise Exception(f\"No such sentence embedding method: {sentence_embedding_method}\")\n",
        "        return sentence_embedding\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJsm9TrnW-bC",
        "colab_type": "text"
      },
      "source": [
        "### Query matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X35nczOW-h5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_query(query: str) -> List[float]:\n",
        "    \"\"\" Vectorize a sentence \"\"\"\n",
        "    pp_query, query_raw = preprocess_text(query, stem_words=False, remove_num=False)\n",
        "    query_vector = embedding_model.compute_sentence_vector(pp_query[0])\n",
        "    return query_vector\n",
        "\n",
        "def get_sentences(db_path: str) -> List[Any]:\n",
        "    \"\"\" Retrieve all sentences \"\"\"\n",
        "    connection = sqlite3.connect(db_path)\n",
        "    cursor = connection.cursor()\n",
        "    command = \"SELECT * FROM sentences\"\n",
        "    cursor.execute(command)\n",
        "    data = cursor.fetchall()\n",
        "    cursor.close()\n",
        "    connection.close()\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_article(db_path: str, paper_doi) -> List[Any]:\n",
        "    \"\"\" Retrieve article by paper_doi \"\"\"\n",
        "    connection = sqlite3.connect(db_path)\n",
        "    cursor = connection.cursor()\n",
        "    command = \"SELECT * FROM articles WHERE paper_doi='%s'\" % paper_doi\n",
        "    cursor.execute(command)\n",
        "    data = cursor.fetchall()\n",
        "    cursor.close()\n",
        "    connection.close()\n",
        "\n",
        "    return data\n",
        "\n",
        "def compute_cosine_distance(args: Any) -> float:\n",
        "    \"\"\" Compute cosine distance between two embeded sentences \"\"\"\n",
        "    sentence_vector = args[1]\n",
        "    query_vector = args[0]\n",
        "    distance = 1 - cosine_similarity([query_vector], [sentence_vector])[0][0]\n",
        "\n",
        "    return (distance, sentence_vector)\n",
        "\n",
        "def query_db_for_sentence(db_path: str, vector: str):\n",
        "    \"\"\" Get a full sentence from a vector \"\"\"\n",
        "    connection = sqlite3.connect(db_path)\n",
        "    cursor = connection.cursor()\n",
        "    command = \"SELECT * FROM sentences WHERE vector='%s'\" % vector\n",
        "    cursor.execute(command)\n",
        "    data = cursor.fetchall()\n",
        "    cursor.close()\n",
        "    connection.close()\n",
        "\n",
        "    data = list(set(data))\n",
        "\n",
        "    if len(data) > 1:\n",
        "      print(f\"ERROR: two sentences with vector {vector} have been found.\")\n",
        "      data = None\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TvvpuDdBYNR",
        "colab_type": "text"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1726qvpmBaDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DB_VERSION = 3                      # V1: no embedded vector, V2 no raw sentence\n",
        "\n",
        "PRE_PROC_STEM_WORDS = False         # Stem words to their root during pre-processing\n",
        "PRE_PROC_REMOVE_NUM = True          # Remove numerical tokens during pre-processing\n",
        "\n",
        "EMBEDDING_DIMENSION = 100           # The word embedding dimension to be used (sentence embedding dim as well).\n",
        "SENTENCE_EMBEDDING_METHOD = \"mowe\"  # How to calculate sentence vector (SUM or MEAN of words embedding): mowe or sowe.\n",
        "\n",
        "# Create a DB file name regarding these parameters\n",
        "today = date.today().strftime(\"%d%m%Y\")\n",
        "DB_FILE_NAME = os.path.join(f\"articles_database_v{DB_VERSION}_{today}_embedding_{EMBEDDING_DIMENSION}_remove_num_{str(PRE_PROC_REMOVE_NUM)}_stem_words_{str(PRE_PROC_STEM_WORDS)}.sqlite\")\n",
        "print(DB_FILE_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_iRtDlpBe0D",
        "colab_type": "text"
      },
      "source": [
        "## Insert articles into sqlite DB¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MbKnVdHBhN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_db_and_load_articles(db_path: str = \"articles_database.sqlite\", load_file: bool = True) -> None:\n",
        "    \"\"\" Load metadata.csv, try to get body texts and insert \"\"\"\n",
        "\n",
        "    if load_file is True:\n",
        "      assert os.path.isfile(db_path)\n",
        "      print(f\"DB {db_path} will be used instead.\")\n",
        "\n",
        "    else:\n",
        "      tic = time.time()\n",
        "\n",
        "      # The metadata.csv file will be used to fetch available files\n",
        "      metadata_path = os.path.join(os.sep, \"content\", \"kaggle_data\", \"metadata.csv\")\n",
        "      metadata_df = pd.read_csv(metadata_path)\n",
        "      # The DOI isn't unique, then let's keep the last version of a duplicated paper\n",
        "      metadata_df.drop_duplicates(subset=[\"doi\"], keep=\"last\", inplace=True)\n",
        "      # Load usefull information to be stored: id, title, body, abstract, date, sha, folder\n",
        "      articles_to_be_inserted = [(article, DB_FILE_NAME) for article in get_articles_to_insert(metadata_df)]\n",
        "      # Create a new SQLite DB file\n",
        "      instanciate_sql_db(db_path=DB_FILE_NAME)\n",
        "      # Parallelize articles insertion\n",
        "      with mp.Pool(os.cpu_count()) as pool:\n",
        "          pool.map(insert_article, articles_to_be_inserted)\n",
        "\n",
        "      toc = time.time()\n",
        "      print(f\"Took {round((toc-tic) / 60, 2)} min to insert {len(articles_to_be_inserted)} articles (SQLite DB: {db_path}).\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8MJLIv4B1Fy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change load_file to False if you want to create the DB for the first time\n",
        "create_db_and_load_articles(DB_FILE_NAME, load_file=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWpVxok47a6-",
        "colab_type": "text"
      },
      "source": [
        "## Load word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjqsCy_C7av9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_model = Embedding(\n",
        "    vectors_path=os.path.join(os.sep, \"content\", \"glove_vectors\", f\"glove.6B.{EMBEDDING_DIMENSION}d.txt\"),\n",
        "    embeddings_dimension=EMBEDDING_DIMENSION,\n",
        "    sentence_embedding_method=SENTENCE_EMBEDDING_METHOD\n",
        ")\n",
        "embedding_model.build_vectors_dictionary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv6ecQF_DnNl",
        "colab_type": "text"
      },
      "source": [
        "## Pre-process and vectorize texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UKFh7aoClZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process_and_vectorize_texts(embedding_model: Embedding, db_path: str = \"articles_database.sqlite\", load_file: bool = True, stem_words: bool = False, remove_num : bool = False) -> None:\n",
        "    \"\"\" Apply pre-processing to all loaded articles \"\"\"\n",
        "\n",
        "    if load_file is True:\n",
        "      assert os.path.isfile(db_path)\n",
        "      print(f\"DB {db_path} will be used instead.\")\n",
        "\n",
        "    else:\n",
        "      tic = time.time()\n",
        "\n",
        "      # Get all previously inserted IDS as well as a pointer on embedding method\n",
        "      ids = [(id_, embedding_model, db_path, stem_words, remove_num) for id_ in get_all_ids(db_path=db_path)]\n",
        "      # For each title, abstract and body, pre-processed found data\n",
        "\n",
        "      with picklable_pool(os.cpu_count()) as pool:\n",
        "          pool.map(pre_process_articles, ids)\n",
        "\n",
        "      toc = time.time()\n",
        "      print(f\"Took {round((toc-tic) / 60, 2)} min to pre-process {len(ids)} articles (SQLite DB: {db_path}).\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN0Chs8CDp0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change load_file to False if you want to create the DB for the first time\n",
        "pre_process_and_vectorize_texts(embedding_model, DB_FILE_NAME, load_file=False, stem_words=PRE_PROC_STEM_WORDS, remove_num=PRE_PROC_REMOVE_NUM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh649HrJDGP5",
        "colab_type": "text"
      },
      "source": [
        "## Query the DB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j3R1CsFKGoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO merge with DB/Query utilities\n",
        "def get_db_sentences_vectors(db_path: str = \"articles_database.sqlite\"):\n",
        "  \"\"\" Get sentences vectors to be matched with queries (stay in RAM, thus computed once).\"\"\"\n",
        "  sentences = get_sentences(db_path)\n",
        "  sentences_vectors = [[float(x) for x in json.loads(sentence_vector[4])] for sentence_vector in sentences]\n",
        "  sentences_vectors = [vector for vector in sentences_vectors if np.nansum(vector) != 0]\n",
        "\n",
        "  print(f\"Queries will be matched versus {len(sentences_vectors)} vectors.\")\n",
        "  return sentences_vectors \n",
        "\n",
        "def get_query_distances_and_vectors(query: str, sentences_vectors):\n",
        "  tic = time.time()\n",
        "\n",
        "  # Vectorize it and format as arguments to be mapped by mp.Pool\n",
        "  query_vector = list(vectorize_query(query))\n",
        "  mapping_arguments = [(query_vector, sentence_vector) for sentence_vector in sentences_vectors]\n",
        "\n",
        "  # Execute\n",
        "  with mp.Pool(os.cpu_count()) as pool:\n",
        "    distances_and_vectors = pool.map(compute_cosine_distance, mapping_arguments)\n",
        "  toc = time.time()\n",
        "  print(f\"Took {round((toc-tic) / 60, 2)} min to process the query.\")\n",
        "\n",
        "  return distances_and_vectors\n",
        "\n",
        "def get_k_closest_sentences(distances_and_vectors, k = 5) -> List:\n",
        "  # Get results\n",
        "  distances = [item[0] for item in distances_and_vectors]\n",
        "  vectors = [item[1] for item in distances_and_vectors]\n",
        "\n",
        "  # Find  k closest \n",
        "  closest_sentence_indexes = np.argpartition(np.array(distances), k)[:k]\n",
        "  closest_vectors = [vectors[idx] for idx in closest_sentence_indexes]\n",
        "  closest_vectors_str = [json.dumps([str(x) for x in vec]) for vec in closest_vectors]\n",
        "\n",
        "  # Retrieve closest sentences\n",
        "  closest_sentences = [query_db_for_sentence(vector=vec_str, db_path=DB_FILE_NAME) for vec_str in closest_vectors_str]\n",
        "\n",
        "  return closest_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftXbGR4_KGtU",
        "colab_type": "code",
        "outputId": "e6d60674-9b22-465c-c91e-bd9f98094ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences_vectors = get_db_sentences_vectors(DB_FILE_NAME)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Queries will be matched versus 344182 vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWufnFIrKGvy",
        "colab_type": "code",
        "outputId": "4a7da2aa-9f25-4778-e9cf-4daa8d27e378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# query = \"chloroquine usage coronavirus treatment\"\n",
        "query = \"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\"\n",
        "\n",
        "distances_and_vectors = get_query_distances_and_vectors(query, sentences_vectors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 1.41 min to process the query.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztz5RCHXKG5Z",
        "colab_type": "code",
        "outputId": "d67891ef-ae46-4658-b612-dcbb0f59d2aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# K best results\n",
        "k = 20\n",
        "closest_sentences = get_k_closest_sentences(distances_and_vectors, k)\n",
        "for sentence in closest_sentences:\n",
        "\n",
        "  print(\"SENTENCE\")\n",
        "  print(f\"\\tSECTION:\\t\\t{sentence[0][1]}\")\n",
        "  print(f\"\\tSENTENCE:\\t\\t{sentence[0][2]}\")\n",
        "  print(f\"\\tVECTOR:\\t\\t\\t{str(sentence[0][3])}\")\n",
        "  # article data \n",
        "  print(\"ARTICLE\")\n",
        "  article = get_article(DB_FILE_NAME, sentence[0][0])\n",
        "  print(f\"\\tTITLE:\\t\\t\\t{str(article[0][4])}\")\n",
        "  print(f\"\\tDOI:\\t\\t\\t{sentence[0][0]}\")\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tTherefore, the objective of this study was to evaluate the survival of PEDV in 9 different feed ingredients when exposed to 60, 70, 80, and 90 °C, as well as the survival on four different surfaces (galvanized steel, stainless steel, aluminum, and plastic).\n",
            "\tVECTOR:\t\t\t[\"therefore\", \"objective\", \"study\", \"evaluate\", \"survival\", \"pedv\", \"different\", \"feed\", \"ingredients\", \"exposed\", \"well\", \"survival\", \"four\", \"different\", \"surfaces\", \"galvanized\", \"steel\", \"stainless\", \"steel\", \"aluminum\", \"plastic\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tSurvival of porcine epidemic diarrhea virus (PEDV) in thermally treated feed ingredients and on surfaces\n",
            "\tDOI:\t\t\t10.1186/s40813-017-0064-3\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tAfter drying, HCoV-229E infectivity was still detectable after 3h on various surfaces (aluminum, sterile latex surgical gloves, sterile sponges) but HCoV-OC43 survived 1h or less.\n",
            "\tVECTOR:\t\t\t[\"drying\", \"hcov\", \"infectivity\", \"still\", \"detectable\", \"various\", \"surfaces\", \"aluminum\", \"sterile\", \"latex\", \"surgical\", \"gloves\", \"sterile\", \"sponges\", \"hcov\", \"oc43\", \"survived\", \"less\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tSurvival of human coronaviruses 229E and OC43 in suspension and after drying onsurfaces: a possible source ofhospital-acquired infections\n",
            "\tDOI:\t\t\t10.1053/jhin.2000.0795\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tPotential applications for the technology are in antimicrobial coatings for, or materials comprising objects, such as tubing, collection bags, handrails, finger-plates on hospital doors, or medical equipment found in the healthcare setting.\n",
            "\tVECTOR:\t\t\t[\"potential\", \"applications\", \"technology\", \"antimicrobial\", \"coatings\", \"materials\", \"comprising\", \"objects\", \"tubing\", \"collection\", \"bags\", \"handrails\", \"finger\", \"plates\", \"hospital\", \"doors\", \"medical\", \"equipment\", \"found\", \"healthcare\", \"setting\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tPhotodynamic Antimicrobial Polymers for Infection Control\n",
            "\tDOI:\t\t\t10.1371/journal.pone.0108500\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tHence, we evaluated the survival of PEDV on inanimate objects routinely used on swine farms such as styrofoam, rubber, plastic, coveralls, and other equipment.\n",
            "\tVECTOR:\t\t\t[\"hence\", \"evaluated\", \"survival\", \"pedv\", \"inanimate\", \"objects\", \"routinely\", \"used\", \"swine\", \"farms\", \"styrofoam\", \"rubber\", \"plastic\", \"coveralls\", \"equipment\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tStability of Porcine Epidemic Diarrhea Virus on Fomite Materials at Different Temperatures\n",
            "\tDOI:\t\t\t10.3390/vetsci5010021\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tMETHODS: Influenza A virus (0.5 mL) was deposited on the surface of a rubber glove, an N95 particulate respirator, a surgical mask made of non-woven fabric, a gown made of Dupont Tyvek, a coated wooden desk, and stainless steel.\n",
            "\tVECTOR:\t\t\t[\"methods\", \"influenza\", \"virus\", \"ml\", \"deposited\", \"surface\", \"rubber\", \"glove\", \"n95\", \"particulate\", \"respirator\", \"surgical\", \"mask\", \"made\", \"non\", \"woven\", \"fabric\", \"gown\", \"made\", \"dupont\", \"tyvek\", \"coated\", \"wooden\", \"desk\", \"stainless\", \"steel\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tMaintenance of influenza virus infectivity on the surfaces of personal protective equipment and clothing used in healthcare settings\n",
            "\tDOI:\t\t\t10.1007/s12199-010-0149-y\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tSurface areas of 100 and 1,000 cm(2) of nonporous fomites found in indoor environments (acrylic, galvanized steel, and laminate) were evaluated with premoistened antistatic wipes.\n",
            "\tVECTOR:\t\t\t[\"surface\", \"areas\", \"cm\", \"nonporous\", \"fomites\", \"found\", \"indoor\", \"environments\", \"acrylic\", \"galvanized\", \"steel\", \"laminate\", \"evaluated\", \"premoistened\", \"antistatic\", \"wipes\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tEvaluation of Sample Recovery Efficiency for Bacteriophage P22 on Fomites\n",
            "\tDOI:\t\t\t10.1128/aem.01370-12\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tThis work presents a simple one-pot protocol to achieve core-doped shell nanohybrids comprising silver nanoparticles, curcumin and thermoresponsive polymeric shell taking advantage of the reducing properties of phenolic curcumin substance and its ability to decorate metallic surfaces.\n",
            "\tVECTOR:\t\t\t[\"work\", \"presents\", \"simple\", \"one\", \"pot\", \"protocol\", \"achieve\", \"core\", \"doped\", \"shell\", \"nanohybrids\", \"comprising\", \"silver\", \"nanoparticles\", \"curcumin\", \"thermoresponsive\", \"polymeric\", \"shell\", \"taking\", \"advantage\", \"reducing\", \"properties\", \"phenolic\", \"curcumin\", \"substance\", \"ability\", \"decorate\", \"metallic\", \"surfaces\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tCurcumin to Promote the Synthesis of Silver NPs and their Self-Assembly with a Thermoresponsive Polymer in Core-Shell Nanohybrids\n",
            "\tDOI:\t\t\t10.1038/s41598-019-54752-4\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tBacterial filtration tests showed that the six-layer filter cloth can clear bacteria effectively, and the porous ceramic can remove 100% of the bacteria.\n",
            "\tVECTOR:\t\t\t[\"bacterial\", \"filtration\", \"tests\", \"showed\", \"six\", \"layer\", \"filter\", \"cloth\", \"clear\", \"bacteria\", \"effectively\", \"porous\", \"ceramic\", \"remove\", \"bacteria\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tPreparation and properties of hydroxyapatite filters for microbial filtration\n",
            "\tDOI:\t\t\t10.1016/j.ceramint.2005.10.014\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tThe most commonly contaminated surfaces were the bed sheet and the towel.\n",
            "\tVECTOR:\t\t\t[\"commonly\", \"contaminated\", \"surfaces\", \"bed\", \"sheet\", \"towel\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tContamination of hospital surfaces with respiratory pathogens in Bangladesh\n",
            "\tDOI:\t\t\t10.1371/journal.pone.0224065\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tWe report here that pathogenic human coronavirus 229E remained infectious in a human lung cell culture model following at least 5 days of persistence on a range of common nonbiocidal surface materials, including polytetrafluoroethylene (Teflon; PTFE), polyvinyl chloride (PVC), ceramic tiles, glass, silicone rubber, and stainless steel.\n",
            "\tVECTOR:\t\t\t[\"report\", \"pathogenic\", \"human\", \"coronavirus\", \"remained\", \"infectious\", \"human\", \"lung\", \"cell\", \"culture\", \"model\", \"following\", \"least\", \"days\", \"persistence\", \"range\", \"common\", \"nonbiocidal\", \"surface\", \"materials\", \"including\", \"polytetrafluoroethylene\", \"teflon\", \"ptfe\", \"polyvinyl\", \"chloride\", \"pvc\", \"ceramic\", \"tiles\", \"glass\", \"silicone\", \"rubber\", \"stainless\", \"steel\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tHuman Coronavirus 229E Remains Infectious on Common Touch Surface Materials\n",
            "\tDOI:\t\t\t10.1128/mbio.01697-15\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tIn this study, a close surrogate of human norovirus causing gastrointestinal disease in mice, murine norovirus type 1 (MNV-1), retained infectivity for more than 2 weeks following contact with a range of surface materials, including Teflon (polytetrafluoroethylene [PTFE]), polyvinyl chloride (PVC), ceramic tiles, glass, silicone rubber, and stainless steel.\n",
            "\tVECTOR:\t\t\t[\"study\", \"close\", \"surrogate\", \"human\", \"norovirus\", \"causing\", \"gastrointestinal\", \"disease\", \"mice\", \"murine\", \"norovirus\", \"type\", \"mnv\", \"retained\", \"infectivity\", \"weeks\", \"following\", \"contact\", \"range\", \"surface\", \"materials\", \"including\", \"teflon\", \"polytetrafluoroethylene\", \"ptfe\", \"polyvinyl\", \"chloride\", \"pvc\", \"ceramic\", \"tiles\", \"glass\", \"silicone\", \"rubber\", \"stainless\", \"steel\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tInactivation of Murine Norovirus on a Range of Copper Alloy Surfaces Is Accompanied by Loss of Capsid Integrity\n",
            "\tDOI:\t\t\t10.1128/aem.03280-14\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tAbstract Immune measures and the fecal bacterial community were examined in female Biobreeding rats housed in wire bottom cages (wire) or in solid bottom cages containing hardwood chips (bedding).\n",
            "\tVECTOR:\t\t\t[\"abstract\", \"immune\", \"measures\", \"fecal\", \"bacterial\", \"community\", \"examined\", \"female\", \"biobreeding\", \"rats\", \"housed\", \"wire\", \"bottom\", \"cages\", \"wire\", \"solid\", \"bottom\", \"cages\", \"containing\", \"hardwood\", \"chips\", \"bedding\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tHousing influences tissue cytokine levels and the fecal bacterial community structure in rats\n",
            "\tDOI:\t\t\t10.1016/j.jff.2017.10.021\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\ttitle\n",
            "\tSENTENCE:\t\tPhysical properties of polymer composite: Natural rubber glove waste/polystyrene foam waste/cellulose\n",
            "\tVECTOR:\t\t\t[\"physical\", \"properties\", \"polymer\", \"composite\", \"natural\", \"rubber\", \"glove\", \"waste\", \"polystyrene\", \"foam\", \"waste\", \"cellulose\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tPhysical properties of polymer composite: Natural rubber glove waste/polystyrene foam waste/cellulose\n",
            "\tDOI:\t\t\t10.1016/j.indcrop.2011.10.017\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tAn apparatus used to grow protein crystals in capillaries and to compare the background X-ray scattering of the components, including thin-walled glass capillaries against Teflon, and various fluorocarbon oils against each other, is described.\n",
            "\tVECTOR:\t\t\t[\"apparatus\", \"used\", \"grow\", \"protein\", \"crystals\", \"capillaries\", \"compare\", \"background\", \"ray\", \"scattering\", \"components\", \"including\", \"thin\", \"walled\", \"glass\", \"capillaries\", \"teflon\", \"various\", \"fluorocarbon\", \"oils\", \"described\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tIn situ data collection and structure refinement from microcapillary protein crystallization\n",
            "\tDOI:\t\t\t10.1107/s002188980502649x\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tAbstract The polymer composite was prepared from the wastes of natural rubber glove (NRG) and polystyrene foam (PSF) blended with cellulose from sugar cane leaves via the laminate method.\n",
            "\tVECTOR:\t\t\t[\"abstract\", \"polymer\", \"composite\", \"prepared\", \"wastes\", \"natural\", \"rubber\", \"glove\", \"nrg\", \"polystyrene\", \"foam\", \"psf\", \"blended\", \"cellulose\", \"sugar\", \"cane\", \"leaves\", \"via\", \"laminate\", \"method\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tPhysical properties of polymer composite: Natural rubber glove waste/polystyrene foam waste/cellulose\n",
            "\tDOI:\t\t\t10.1016/j.indcrop.2011.10.017\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tFinally, PEDV survival was similar on galvanized steel, stainless steel, aluminum and plastic.\n",
            "\tVECTOR:\t\t\t[\"finally\", \"pedv\", \"survival\", \"similar\", \"galvanized\", \"steel\", \"stainless\", \"steel\", \"aluminum\", \"plastic\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tSurvival of porcine epidemic diarrhea virus (PEDV) in thermally treated feed ingredients and on surfaces\n",
            "\tDOI:\t\t\t10.1186/s40813-017-0064-3\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tA more sensitive immunoplaque assay was able to detect virus from Styrofoam, metal, and plastic at 20 days post application, representing a 3-log loss of input virus on fomite materials.\n",
            "\tVECTOR:\t\t\t[\"sensitive\", \"immunoplaque\", \"assay\", \"able\", \"detect\", \"virus\", \"styrofoam\", \"metal\", \"plastic\", \"days\", \"post\", \"application\", \"representing\", \"log\", \"loss\", \"input\", \"virus\", \"fomite\", \"materials\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tStability of Porcine Epidemic Diarrhea Virus on Fomite Materials at Different Temperatures\n",
            "\tDOI:\t\t\t10.3390/vetsci5010021\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tPathogenic bacteria also penetrate food production areas and may remain there in the form of a biofilm covering the surfaces of machines and equipment.\n",
            "\tVECTOR:\t\t\t[\"pathogenic\", \"bacteria\", \"also\", \"penetrate\", \"food\", \"production\", \"areas\", \"may\", \"remain\", \"form\", \"biofilm\", \"covering\", \"surfaces\", \"machines\", \"equipment\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tCampylobacteriosis, Salmonellosis, Yersiniosis, and Listeriosis as Zoonotic Foodborne Diseases: A Review\n",
            "\tDOI:\t\t\t10.3390/ijerph15050863\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tElectron microscopy of purified MNV-1 that had been exposed to copper and stainless steel surfaces suggested that a massive breakdown of the viral capsid had occurred on copper.\n",
            "\tVECTOR:\t\t\t[\"electron\", \"microscopy\", \"purified\", \"mnv\", \"exposed\", \"copper\", \"stainless\", \"steel\", \"surfaces\", \"suggested\", \"massive\", \"breakdown\", \"viral\", \"capsid\", \"occurred\", \"copper\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tInactivation of Murine Norovirus on a Range of Copper Alloy Surfaces Is Accompanied by Loss of Capsid Integrity\n",
            "\tDOI:\t\t\t10.1128/aem.03280-14\n",
            "\n",
            "\n",
            "SENTENCE\n",
            "\tSECTION:\t\tabstract\n",
            "\tSENTENCE:\t\tAbstract Filtration of aerosol particles using non-woven fibrous media is a common practice for air cleaning.\n",
            "\tVECTOR:\t\t\t[\"abstract\", \"filtration\", \"aerosol\", \"particles\", \"using\", \"non\", \"woven\", \"fibrous\", \"media\", \"common\", \"practice\", \"air\", \"cleaning\"]\n",
            "ARTICLE\n",
            "\tTITLE:\t\t\tAerosol Filtration Application Using Fibrous Media—An Industrial Perspective\n",
            "\tDOI:\t\t\t10.1016/s1004-9541(12)60356-5\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8PP1aNFHuKq",
        "colab_type": "text"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HTSDId-LFIC",
        "colab_type": "code",
        "outputId": "b99fda9e-acb3-4339-ccd9-47f92c39677b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "connection = sqlite3.connect(DB_FILE_NAME)\n",
        "cursor = connection.cursor()\n",
        "cursor.execute(\"SELECT * FROM sentences\")\n",
        "res = cursor.fetchall()\n",
        "cursor.close()\n",
        "connection.close()\n",
        "\n",
        "print(len(res))\n",
        "print(\"\\n\".join(res[32]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "344482\n",
            "10.1001/jama.2014.2116\n",
            "title\n",
            "Critically Ill Patients With Influenza A(H1N1)pdm09 Virus Infection in 2014\n",
            "[\"critically\", \"ill\", \"patients\", \"influenza\", \"h1n1\", \"pdm09\", \"virus\", \"infection\"]\n",
            "[\"-0.03297171428571429\", \"-0.07034200000000003\", \"-0.13139299999999998\", \"0.23254414285714284\", \"-0.21820857142857145\", \"0.25680571428571425\", \"0.4409482857142857\", \"-0.28643142857142856\", \"0.3511464285714286\", \"0.02130357142857142\", \"-0.1704287142857143\", \"0.30054571428571425\", \"0.3011331428571428\", \"-0.14776571428571428\", \"0.14185214285714287\", \"0.30474285714285715\", \"-0.310925\", \"-0.14698807142857143\", \"0.2134452142857143\", \"-0.002152714285714254\", \"0.04621328571428572\", \"-0.07467785714285714\", \"-0.1030742857142857\", \"0.20810857142857145\", \"-0.016704285714285727\", \"0.17449864285714287\", \"0.3705238571428571\", \"0.13111942857142855\", \"-0.09994071428571427\", \"0.3934417142857143\", \"0.088722\", \"0.07798914285714284\", \"-0.025241000000000006\", \"0.17041\", \"0.1196057142857143\", \"-0.20598457142857146\", \"-0.21148485714285714\", \"-0.3690187142857143\", \"-0.03932142857142858\", \"0.319206\", \"0.07965285714285718\", \"0.5074287142857142\", \"-0.26191714285714285\", \"-0.1462574285714286\", \"0.612814\", \"0.45344357142857133\", \"0.16526\", \"0.09424928571428569\", \"-0.20620499999999997\", \"-0.10817685714285716\", \"0.33863571428571426\", \"-0.1472532857142857\", \"-0.0989942857142857\", \"-0.018772857142857122\", \"0.062191857142857146\", \"0.10113428571428577\", \"0.07146142857142856\", \"-0.16622471428571428\", \"-0.0541171428571429\", \"0.06693585714285713\", \"0.116268\", \"0.4962237\", \"0.32968085714285716\", \"-0.2115232857142857\", \"0.025978571428571424\", \"0.44015000000000004\", \"-0.24115\", \"-0.4864685714285714\", \"0.13044314285714287\", \"-0.09073200000000001\", \"-0.18621554285714284\", \"-0.12280757142857143\", \"-0.020289785714285715\", \"0.48004285714285716\", \"0.3062671428571428\", \"0.12949285714285716\", \"-0.2657297142857143\", \"-0.1547892857142857\", \"0.017977142857142834\", \"-0.2299\", \"0.08744171428571428\", \"0.2508671428571429\", \"0.07374285714285718\", \"0.02232242857142857\", \"-0.48193\", \"-0.014471714285714274\", \"0.20346857142857142\", \"0.2665814285714286\", \"-0.6350341428571429\", \"0.3337461428571428\", \"0.18704357142857142\", \"-0.11195830000000002\", \"0.126754\", \"-0.124525\", \"-0.06245971428571425\", \"0.1892342857142857\", \"0.09001885714285715\", \"-0.0926085714285714\", \"-0.06758457142857144\", \"-0.4562853857142857\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "873WZeEDEeDv",
        "colab_type": "code",
        "outputId": "812078da-b32d-43b5-fef0-e94eace32a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "sentence_1 = [\"cool\", \"concert\", \"guitar\"]\n",
        "sentence_2 = [\"super\", \"piano\", \"song\"]\n",
        "sentence_3 = [\"boat\", \"drugs\", \"corona\"]\n",
        "\n",
        "distance_1_2 = compute_cosine_distance(\n",
        "    embeding_model.compute_sentence_vector(sentence_1),\n",
        "    embeding_model.compute_sentence_vector(sentence_2)\n",
        ")\n",
        "\n",
        "distance_1_3 = compute_cosine_distance(\n",
        "    embeding_model.compute_sentence_vector(sentence_1),\n",
        "    embeding_model.compute_sentence_vector(sentence_3)\n",
        ")\n",
        "\n",
        "print(f\"Distance between: '{' '.join(sentence_1)}' and '{' '.join(sentence_2)}': {distance_1_2}\")\n",
        "print(f\"Distance between: '{' '.join(sentence_1)}' and '{' '.join(sentence_3)}': {distance_1_3}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-dbad9fd204fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m distance_1_2 = compute_cosine_distance(\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0membeding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_sentence_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0membeding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_sentence_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embeding_model' is not defined"
          ]
        }
      ]
    }
  ]
}